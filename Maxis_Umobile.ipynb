{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Maxis*Umobile.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Variables that contains the user credentials to access Twitter API\n",
        "access_token = \"1199209563309793280-Y7AiatPCwut2mmIt6pUBejikbWCxqQ\"\n",
        "access_token_secret = \"Vbjkmz1EJBfk8q7oyfA9YDo8lMSRfT7byWnKIQoYXzuLv\"\n",
        "consumer_key = \"TRC7XJbMlAZ1owuEFKLeT8Btf\"\n",
        "consumer_secret = \"eWAtVMPHxPFCtMKmBOvheLtbYbPUikaPsCYmQ8EW575cWN7F9x\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tweepy'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-2-c35b42615b4c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtweepy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOAuthHandler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# implement a simple dashboard to visuallize all relevant metrics that must be monitored and measured.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tweepy'"
          ]
        }
      ],
      "source": [
        "import tweepy\n",
        "from tweepy import OAuthHandler\n",
        "\n",
        "auth = OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "screen_name =\"Maxis\"\n",
        "\n",
        "def basicUserAttributes():\n",
        "\n",
        "    user = api.get_user(screen_name)\n",
        "\n",
        "    print(\"Screen name: \", user.screen_name)\n",
        "    print(\"User_id:\", user.id)\n",
        "    print(\"Description:\", user.description)\n",
        "    print(\"Language:\", user.lang)\n",
        "    print(\"Account created at:\", user.created_at)\n",
        "    print(\"Location:\", user.location)\n",
        "    print(\"Number of Tweets:\",user.statuses_count)\n",
        "    print(\"Number of Followers:\", user.followers_count)\n",
        "    print(\"Number of Friends:\", user.friends_count)\n",
        "\n",
        "def UserStatuses():   \n",
        "    '''\n",
        "    print(\"----- <HOME TIMELINE> -----\")\n",
        "    public_tweets = api.home_timeline(30)\n",
        "    for tweet in public_tweets:\n",
        "        print(tweet.text)\n",
        "        print(\"Created at:\",tweet.created_at)\n",
        "        print(\"==========\")\n",
        "    print(\"------ </HOME TIMELINE> -----\")\n",
        "    print(\"\\n\")\n",
        "    '''\n",
        "\n",
        "    \n",
        "    print(\"----- <USER TIMELINE> -----\")\n",
        "    user_tweets = api.user_timeline(screen_name,count=1000)   \n",
        "    count = 0\n",
        "    platform = {}\n",
        "    \n",
        "    for tweet in user_tweets:\n",
        "        #print(tweet.text)  \n",
        "        count += 1\n",
        "        if tweet.source not in platform.keys():\n",
        "            platform[tweet.source] = 1\n",
        "        else:\n",
        "            platform[tweet.source] += 1\n",
        "    print(\"Count:\",count)\n",
        "    most_used_platform = [i for i in platform.keys() if platform[i]==max(platform.values())][0]\n",
        "    start = user_tweets[0].created_at\n",
        "    end = user_tweets[count-1].created_at\n",
        "    total_time = (start-end).total_seconds()/(60*60*24)\n",
        "    print(\"Seconds:\",(start-end).total_seconds())\n",
        "    print(screen_name,\"tweets\",(round((count/total_time),2)),\"posts per day, mostly via\",most_used_platform)\n",
        "    \n",
        "    \n",
        "    print(\"------ </USER TIMELINE> ------\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    basicUserAttributes()\n",
        "    print()\n",
        "    UserStatuses()\n",
        "\n",
        "screen_name =\"umobile\"\n",
        "\n",
        "def basicUserAttributes():\n",
        "\n",
        "    user = api.get_user(screen_name)\n",
        "\n",
        "    print(\"Screen name: \", user.screen_name)\n",
        "    print(\"User_id:\", user.id)\n",
        "    print(\"Description:\", user.description)\n",
        "    print(\"Language:\", user.lang)\n",
        "    print(\"Account created at:\", user.created_at)\n",
        "    print(\"Location:\", user.location)\n",
        "    print(\"Number of Tweets:\",user.statuses_count)\n",
        "    print(\"Number of Followers:\", user.followers_count)\n",
        "    print(\"Number of Friends:\", user.friends_count)\n",
        "\n",
        "def UserStatuses():   \n",
        "    '''\n",
        "    print(\"----- <HOME TIMELINE> -----\")\n",
        "    public_tweets = api.home_timeline(30)\n",
        "    for tweet in public_tweets:\n",
        "        print(tweet.text)\n",
        "        print(\"Created at:\",tweet.created_at)\n",
        "        print(\"==========\")\n",
        "    print(\"------ </HOME TIMELINE> -----\")\n",
        "    print(\"\\n\")\n",
        "    '''\n",
        "\n",
        "    \n",
        "    print(\"----- <USER TIMELINE> -----\")\n",
        "    user_tweets = api.user_timeline(screen_name,count=1000)   \n",
        "    count = 0\n",
        "    platform = {}\n",
        "    \n",
        "    for tweet in user_tweets:\n",
        "        #print(tweet.text)  \n",
        "        count += 1\n",
        "        if tweet.source not in platform.keys():\n",
        "            platform[tweet.source] = 1\n",
        "        else:\n",
        "            platform[tweet.source] += 1\n",
        "    print(\"Count:\",count)\n",
        "    most_used_platform = [i for i in platform.keys() if platform[i]==max(platform.values())][0]\n",
        "    start = user_tweets[0].created_at\n",
        "    end = user_tweets[count-1].created_at\n",
        "    total_time = (start-end).total_seconds()/(60*60*24)\n",
        "    print(\"Seconds:\",(start-end).total_seconds())\n",
        "    print(screen_name,\"tweets\",(round((count/total_time),2)),\"posts per day, mostly via\",most_used_platform)\n",
        "    \n",
        "    \n",
        "    print(\"------ </USER TIMELINE> ------\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    basicUserAttributes()\n",
        "    print()\n",
        "    UserStatuses()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "Maxis and Umobile Twitter User Details and tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EW7PYjVcneHM"
      },
      "source": [
        "Maxis Followers and Friends"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import math\n",
        "from tweepy import Cursor\n",
        "import tweepy\n",
        "from tweepy import OAuthHandler\n",
        "import datetime\n",
        "MAX_FRIENDS = 15000\n",
        "\n",
        "def paginate(items, n):\n",
        "    \"\"\"Generate n-sized chunks from items\"\"\"\n",
        "    for i in range(0, len(items), n):\n",
        "        yield items[i:i+n]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    screen_name = \"Maxis\"\n",
        "    print(\"collecting data for \" + screen_name)\n",
        "    dirname = \"output/users/{}\".format(screen_name)\n",
        "    max_pages = math.ceil(MAX_FRIENDS / 5000)\n",
        "    try:\n",
        "        os.makedirs(dirname, mode=0o755, exist_ok=True)\n",
        "    except OSError:\n",
        "        print(\"Directory {} already exists\".format(dirname))\n",
        "    except Exception as e:\n",
        "        print(\"Error while creating directory {}\".format(dirname))\n",
        "        print(e)\n",
        "        sys.exit(1)\n",
        "\n",
        "    # get Maxis's profile\n",
        "    fname = \"output/users/{}/user_profile.json\".format(screen_name)\n",
        "    with open(fname, 'w') as f:\n",
        "        profile = api.get_user(screen_name=screen_name)        \n",
        "        f.write(json.dumps(profile._json, indent=4))\n",
        "\n",
        "    #extract tweets based on start and end dates - user timeline      \n",
        "    startDate = datetime.datetime(2019, 12, 1) #The date contains year, month, day\n",
        "    endDate =   datetime.datetime(2019, 12, 22)\n",
        "\n",
        "    fname = \"output/users/{}/user_timeline.json\".format(screen_name)\n",
        "    count = 0\n",
        "    with open(fname, 'w') as f:\n",
        "        for page in Cursor(api.user_timeline, screen_name=screen_name, count=200).pages(16):\n",
        "            for status in page:\n",
        "                if status.created_at > startDate and status.created_at < endDate:\n",
        "                    f.write(json.dumps(status.text)+\"\\n\")\n",
        "                    count+=1\n",
        "    print('tweets collected between {}-{}: {}'.format(startDate,endDate,count))\n",
        "    \n",
        "    print()\n",
        "    # get followers for Maxis\n",
        "    fname = \"output/users/{}/followers.json\".format(screen_name)\n",
        "    count = 0\n",
        "    with open(fname, 'w') as f:\n",
        "        for followers in Cursor(api.followers_ids, screen_name=screen_name).pages(max_pages):\n",
        "            stop = False\n",
        "            for chunk in paginate(followers, 100):\n",
        "              try:\n",
        "                users = api.lookup_users(user_ids=chunk)\n",
        "                for user in users:                    \n",
        "                    f.write(json.dumps(user._json)+\"\\n\")\n",
        "                    count += 1\n",
        "              except:\n",
        "                  stop = True\n",
        "                  break                    \n",
        "            if len(followers) == 3000:\n",
        "                print(\"More results available. Sleeping for 60 seconds to avoid rate limit\")\n",
        "                time.sleep(60)\n",
        "            if stop:\n",
        "                break \n",
        "    print('followers count:',count)\n",
        "    count = 0  \n",
        "  \n",
        "    # get friends for Maxis\n",
        "    fname = \"output/users/{}/friends.json\".format(screen_name)\n",
        "    with open(fname, 'w') as f:\n",
        "        for friends in Cursor(api.friends_ids, screen_name=screen_name).pages(max_pages):\n",
        "            stop = False\n",
        "            for chunk in paginate(friends, 100):\n",
        "                try:\n",
        "                  users = api.lookup_users(user_ids=chunk)\n",
        "                  for user in users:                   \n",
        "                      f.write(json.dumps(user._json)+\"\\n\")\n",
        "                      count += 1\n",
        "                except:\n",
        "                    stop = True\n",
        "                    break\n",
        "            if len(friends) == 3000:\n",
        "                print(\"More results available. Sleeping for 60 seconds to avoid rate limit\")\n",
        "                time.sleep(60)\n",
        "            if stop:\n",
        "                break\n",
        "    print('friends count:' ,count)\n",
        "    print(\"task completed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q2sQ0B6EnxTo"
      },
      "source": [
        "Umobile Followers and Friends"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import math\n",
        "from tweepy import Cursor\n",
        "import tweepy\n",
        "from tweepy import OAuthHandler\n",
        "import datetime\n",
        "MAX_FRIENDS = 15000\n",
        "\n",
        "def paginate(items, n):\n",
        "    \"\"\"Generate n-sized chunks from items\"\"\"\n",
        "    for i in range(0, len(items), n):\n",
        "        yield items[i:i+n]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    screen_name = \"umobile\"\n",
        "    print(\"collecting data for \" + screen_name)\n",
        "    dirname = \"output/users/{}\".format(screen_name)\n",
        "    max_pages = math.ceil(MAX_FRIENDS / 5000)\n",
        "    try:\n",
        "        os.makedirs(dirname, mode=0o755, exist_ok=True)\n",
        "    except OSError:\n",
        "        print(\"Directory {} already exists\".format(dirname))\n",
        "    except Exception as e:\n",
        "        print(\"Error while creating directory {}\".format(dirname))\n",
        "        print(e)\n",
        "        sys.exit(1)\n",
        "\n",
        "    # get Maxis's profile\n",
        "    fname = \"output/users/{}/user_profile.json\".format(screen_name)\n",
        "    with open(fname, 'w') as f:\n",
        "        profile = api.get_user(screen_name=screen_name)        \n",
        "        f.write(json.dumps(profile._json, indent=4))\n",
        "\n",
        "    #extract tweets based on start and end dates - user timeline      \n",
        "    startDate = datetime.datetime(2019, 12, 1) #The date contains year, month, day\n",
        "    endDate =   datetime.datetime(2019, 12, 22)\n",
        "\n",
        "    fname = \"output/users/{}/user_timeline.json\".format(screen_name)\n",
        "    count = 0\n",
        "    with open(fname, 'w') as f:\n",
        "        for page in Cursor(api.user_timeline, screen_name=screen_name, count=200).pages(16):\n",
        "            for status in page:\n",
        "                if status.created_at > startDate and status.created_at < endDate:\n",
        "                    f.write(json.dumps(status.text)+\"\\n\")\n",
        "                    count+=1\n",
        "    print('tweets collected between {}-{}: {}'.format(startDate,endDate,count))\n",
        "    \n",
        "    print()\n",
        "    # get followers for Maxis\n",
        "    fname = \"output/users/{}/followers.json\".format(screen_name)\n",
        "    count = 0\n",
        "    with open(fname, 'w') as f:\n",
        "        for followers in Cursor(api.followers_ids, screen_name=screen_name).pages(max_pages):\n",
        "            stop = False\n",
        "            for chunk in paginate(followers, 100):\n",
        "              try:\n",
        "                users = api.lookup_users(user_ids=chunk)\n",
        "                for user in users:                    \n",
        "                    f.write(json.dumps(user._json)+\"\\n\")\n",
        "                    count += 1\n",
        "              except:\n",
        "                  stop = True\n",
        "                  break                    \n",
        "            if len(followers) == 3000:\n",
        "                print(\"More results available. Sleeping for 60 seconds to avoid rate limit\")\n",
        "                time.sleep(60)\n",
        "            if stop:\n",
        "                break \n",
        "    print('followers count:',count)\n",
        "    count = 0  \n",
        "  \n",
        "    # get friends for Maxis\n",
        "    fname = \"output/users/{}/friends.json\".format(screen_name)\n",
        "    with open(fname, 'w') as f:\n",
        "        for friends in Cursor(api.friends_ids, screen_name=screen_name).pages(max_pages):\n",
        "            stop = False\n",
        "            for chunk in paginate(friends, 100):\n",
        "                try:\n",
        "                  users = api.lookup_users(user_ids=chunk)\n",
        "                  for user in users:                   \n",
        "                      f.write(json.dumps(user._json)+\"\\n\")\n",
        "                      count += 1\n",
        "                except:\n",
        "                    stop = True\n",
        "                    break\n",
        "            if len(friends) == 3000:\n",
        "                print(\"More results available. Sleeping for 60 seconds to avoid rate limit\")\n",
        "                time.sleep(60)\n",
        "            if stop:\n",
        "                break\n",
        "    print('friends count:' ,count)\n",
        "    print(\"task completed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kZ_8FjwKpJMG"
      },
      "source": [
        "Maxis distribution graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from random import sample\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    screen_name = \"Maxis\"\n",
        "    followers_file = 'output/users/{}/followers.json'.format(screen_name)\n",
        "    friends_file = 'output/users/{}/friends.json'.format(screen_name)\n",
        "    followers_created_date = []\n",
        "    friends_created_date = []\n",
        "    \n",
        "    with open(followers_file) as f1, open(friends_file) as f2:\n",
        "        followers = []\n",
        "        for line in f1:\n",
        "            profile = json.loads(line)\n",
        "            followers_created_date.append(profile['created_at'])\n",
        "            #break\n",
        "        for line in f2:\n",
        "            profile = json.loads(line)\n",
        "            friends_created_date.append(profile['created_at'])\n",
        "            #break\n",
        "            \n",
        "    month = ['dud','Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\n",
        "    #followers_date = [datetime.datetime(int(i.split(' ')[-1]),month.index(i.split(' ')[1]),1) for i in followers_created_date]\n",
        "    followers_year = [int(i.split(' ')[-1]) for i in followers_created_date]\n",
        "    friends_month = [i.split(' ')[1] for i in friends_created_date]\n",
        "    friends_year = [i.split(' ')[-1] for i in friends_created_date]\n",
        "    \n",
        "    #followers_df = pd.DataFrame({'date':followers_date,'year':followers_year})\n",
        "    followers_df = pd.DataFrame({'year':followers_year})\n",
        "    followers_df = followers_df.sort_values(by='year')\n",
        "    friends_df = pd.DataFrame({'year':friends_year})\n",
        "    friends_df = friends_df.sort_values(by='year')\n",
        "    \n",
        "    followers_df['year'].hist(figsize=(15,5),bins=12,grid=False)\n",
        "    plt.title('Maxis followers created time distribution by year')\n",
        "    plt.show()\n",
        "    friends_df['year'].hist(figsize=(15,5),bins=12,grid=False)\n",
        "    plt.title('Maxis friends created time distribution by year')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DzHjLXg8pSRN"
      },
      "source": [
        "Umobile distribution graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    screen_name = \"umobile\"\n",
        "    followers_file = 'output/users/{}/followers.json'.format(screen_name)\n",
        "    friends_file = 'output/users/{}/friends.json'.format(screen_name)\n",
        "    followers_created_date = []\n",
        "    friends_created_date = []\n",
        "    \n",
        "    with open(followers_file) as f1, open(friends_file) as f2:\n",
        "        followers = []\n",
        "        for line in f1:\n",
        "            profile = json.loads(line)\n",
        "            followers_created_date.append(profile['created_at'])\n",
        "            #break\n",
        "        for line in f2:\n",
        "            profile = json.loads(line)\n",
        "            friends_created_date.append(profile['created_at'])\n",
        "            #break\n",
        "            \n",
        "    month = ['dud','Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\n",
        "    #followers_date = [datetime.datetime(int(i.split(' ')[-1]),month.index(i.split(' ')[1]),1) for i in followers_created_date]\n",
        "    followers_year = [int(i.split(' ')[-1]) for i in followers_created_date]\n",
        "    friends_month = [i.split(' ')[1] for i in friends_created_date]\n",
        "    friends_year = [i.split(' ')[-1] for i in friends_created_date]\n",
        "    \n",
        "    #followers_df = pd.DataFrame({'date':followers_date,'year':followers_year})\n",
        "    followers_df = pd.DataFrame({'year':followers_year})\n",
        "    followers_df = followers_df.sort_values(by='year')\n",
        "    friends_df = pd.DataFrame({'year':friends_year})\n",
        "    friends_df = friends_df.sort_values(by='year')\n",
        "    \n",
        "    followers_df['year'].hist(figsize=(15,5),bins=12,grid=False)\n",
        "    plt.title('Umobile followers created time distribution by year')\n",
        "    plt.show()\n",
        "    friends_df['year'].hist(figsize=(15,5),bins=12,grid=False)\n",
        "    plt.title('Umobile friends created time distribution by year')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ibNpjWUyp9Lg"
      },
      "source": [
        "Maxis Twitter Follower Stat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import json\n",
        "from random import sample\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    screen_name = \"Maxis\"\n",
        "    followers_file = 'output/users/{}/followers.json'.format(screen_name)\n",
        "    friends_file = 'output/users/{}/friends.json'.format(screen_name)\n",
        "    with open(followers_file) as f1, open(friends_file) as f2:\n",
        "        followers = []\n",
        "        friends = []\n",
        "        ffc = []\n",
        "        for line in f1:\n",
        "            profile = json.loads(line)\n",
        "            followers.append(profile['screen_name'])\n",
        "        for line in f2:\n",
        "            profile = json.loads(line)\n",
        "            friends.append(profile['screen_name'])\n",
        "            ffc.append(profile['followers_count'])\n",
        "      \n",
        "        mutual_friends = [user for user in friends if user in followers]\n",
        "        followers_not_following = [user for user in followers if user not in friends]\n",
        "        friends_not_following = [user for user in friends if user not in followers]\n",
        "        \n",
        "        percentage_fnfb = round(len(friends_not_following)*100/len(friends),3)\n",
        "        \n",
        "        # find followers of each fnfb\n",
        "        count = 0\n",
        "        fnfb_centr = []\n",
        "        for u in friends_not_following:\n",
        "            fnfb_centr.append(round(ffc[friends.index(u)]/(len(friends_not_following)-1),2))\n",
        "\n",
        "        fnfb_df = pd.DataFrame({'name':friends_not_following,'centr_deg':fnfb_centr})\n",
        "        fnfb_df = fnfb_df.sort_values(by='centr_deg', ascending=False)\n",
        "        print(fnfb_df[:5])\n",
        "        print(\"----- Stats -----\")\n",
        "        print(\"{} has {} followers\".format(screen_name, len(followers)))\n",
        "        print(\"{} has {} friends\".format(screen_name, len(friends)))\n",
        "        print(\"{} has {} mutual friends\".format(screen_name, len(mutual_friends)))\n",
        "        print(\"{} friends are not following {} back\".format(len(friends_not_following), screen_name))\n",
        "        print(\"{}% of friends are not following back\".format(percentage_fnfb))\n",
        "        print(\"{} followers are not followed back by {}\".format(len(followers_not_following), screen_name))\n",
        "\n",
        "        some_mutual_friends = ', '.join(mutual_friends)\n",
        "        print(\"Some mutual friends: {}\".format(some_mutual_friends))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8-BRmxeZqJAH"
      },
      "source": [
        "Umobile Twitter Follower Stat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import json\n",
        "from random import sample\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    screen_name = \"umobile\"\n",
        "    followers_file = 'output/users/{}/followers.json'.format(screen_name)\n",
        "    friends_file = 'output/users/{}/friends.json'.format(screen_name)\n",
        "    with open(followers_file) as f1, open(friends_file) as f2:\n",
        "        followers = []\n",
        "        friends = []\n",
        "        ffc = []\n",
        "        for line in f1:\n",
        "            profile = json.loads(line)\n",
        "            followers.append(profile['screen_name'])\n",
        "        for line in f2:\n",
        "            profile = json.loads(line)\n",
        "            friends.append(profile['screen_name'])\n",
        "            ffc.append(profile['followers_count'])\n",
        "      \n",
        "        mutual_friends = [user for user in friends if user in followers]\n",
        "        followers_not_following = [user for user in followers if user not in friends]\n",
        "        friends_not_following = [user for user in friends if user not in followers]\n",
        "        \n",
        "        percentage_fnfb = round(len(friends_not_following)*100/len(friends),3)\n",
        "        \n",
        "        # find followers of each fnfb\n",
        "        count = 0\n",
        "        fnfb_centr = []\n",
        "        for u in friends_not_following:\n",
        "            fnfb_centr.append(round(ffc[friends.index(u)]/(len(friends_not_following)-1),2))\n",
        "\n",
        "        fnfb_df = pd.DataFrame({'name':friends_not_following,'centr_deg':fnfb_centr})\n",
        "        fnfb_df = fnfb_df.sort_values(by='centr_deg', ascending=False)\n",
        "        print(fnfb_df[:5])\n",
        "        print(\"----- Stats -----\")\n",
        "        print(\"{} has {} followers\".format(screen_name, len(followers)))\n",
        "        print(\"{} has {} friends\".format(screen_name, len(friends)))\n",
        "        print(\"{} has {} mutual friends\".format(screen_name, len(mutual_friends)))\n",
        "        print(\"{} friends are not following {} back\".format(len(friends_not_following), screen_name))\n",
        "        print(\"{}% of friends are not following back\".format(percentage_fnfb))\n",
        "        print(\"{} followers are not followed back by {}\".format(len(followers_not_following), screen_name))\n",
        "\n",
        "        some_mutual_friends = ', '.join(mutual_friends)\n",
        "        print(\"Some mutual friends: {}\".format(some_mutual_friends))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KJqtGXcGrA5H"
      },
      "source": [
        "Collect Maxis user timeline data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import json\n",
        "from tweepy import Cursor\n",
        "import tweepy\n",
        "from tweepy import OAuthHandler\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    user = \"Maxis\" \n",
        "    fname = \"output/user_timeline_{}.json\".format(user)\n",
        "    print(\"Collecting timeline statuses for\", user)\n",
        "    with open(fname, 'w') as f:\n",
        "        for page in Cursor(api.user_timeline, screen_name=user, count=200).pages(16):\n",
        "            for status in page:                \n",
        "                f.write(json.dumps(status._json)+\"\\n\")\n",
        "\n",
        "    print(\"User Timeline Tweets collected\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BwT8AujerEB8"
      },
      "source": [
        "Collect Umobile user timeline data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import json\n",
        "from tweepy import Cursor\n",
        "import tweepy\n",
        "from tweepy import OAuthHandler\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    user = \"umobile\" \n",
        "    fname = \"output/user_timeline_{}.json\".format(user)\n",
        "    print(\"Collecting timeline statuses for\", user)\n",
        "    with open(fname, 'w') as f:\n",
        "        for page in Cursor(api.user_timeline, screen_name=user, count=200).pages(16):\n",
        "            for status in page:                \n",
        "                f.write(json.dumps(status._json)+\"\\n\")\n",
        "\n",
        "    print(\"User Timeline Tweets collected\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FqUMWtc-rGN3"
      },
      "source": [
        "Maxis Twitter hashtag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "def get_hashtags(tweet):\n",
        "    entities = tweet.get('entities', {})\n",
        "    hashtags = entities.get('hashtags', [])\n",
        "    return [tag['text'].lower() for tag in hashtags]\n",
        "\n",
        "if __name__ == '__main__':    \n",
        "    fname = \"output/user_timeline_Maxis.json\"\n",
        "    with open(fname, 'r') as f:\n",
        "        hashtags = Counter()\n",
        "        for line in f:\n",
        "            tweet = json.loads(line)\n",
        "            hashtags_in_tweet = get_hashtags(tweet)\n",
        "            hashtags.update(hashtags_in_tweet)\n",
        "        for tag, count in hashtags.most_common(20):\n",
        "            print(\"{}: {}\".format(tag, count))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PRBqB-7srhyi"
      },
      "source": [
        "Umobile Twitter hashtag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "def get_hashtags(tweet):\n",
        "    entities = tweet.get('entities', {})\n",
        "    hashtags = entities.get('hashtags', [])\n",
        "    return [tag['text'].lower() for tag in hashtags]\n",
        "\n",
        "if __name__ == '__main__':    \n",
        "    fname = \"output/user_timeline_umobile.json\"\n",
        "    with open(fname, 'r') as f:\n",
        "        hashtags = Counter()\n",
        "        for line in f:\n",
        "            tweet = json.loads(line)\n",
        "            hashtags_in_tweet = get_hashtags(tweet)\n",
        "            hashtags.update(hashtags_in_tweet)\n",
        "        for tag, count in hashtags.most_common(20):\n",
        "            print(\"{}: {}\".format(tag, count))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MAFZ6SyBr4EJ"
      },
      "source": [
        "Maxis twitter mention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "def get_mentions(tweet):\n",
        "    entities = tweet.get('entities', {})\n",
        "    mentions = entities.get('user_mentions', [])\n",
        "    return [tag['screen_name'] for tag in mentions]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    fname = \"output/user_timeline_Maxis.json\"\n",
        "    with open(fname, 'r') as f:\n",
        "        users = Counter()\n",
        "        for line in f:\n",
        "            tweet = json.loads(line)\n",
        "            mentions_in_tweet = get_mentions(tweet)\n",
        "            users.update(mentions_in_tweet)\n",
        "        for user, count in users.most_common(20):\n",
        "            print(\"{}: {}\".format(user, count))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "67dzVAwesPnQ"
      },
      "source": [
        "Umobile twitter mention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "def get_mentions(tweet):\n",
        "    entities = tweet.get('entities', {})\n",
        "    mentions = entities.get('user_mentions', [])\n",
        "    return [tag['screen_name'] for tag in mentions]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    fname = \"output/user_timeline_umobile.json\"\n",
        "    with open(fname, 'r') as f:\n",
        "        users = Counter()\n",
        "        for line in f:\n",
        "            tweet = json.loads(line)\n",
        "            mentions_in_tweet = get_mentions(tweet)\n",
        "            users.update(mentions_in_tweet)\n",
        "        for user, count in users.most_common(20):\n",
        "            print(\"{}: {}\".format(user, count))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G2lKuWYbtTIp"
      },
      "source": [
        "Maxis twitter User Retweet count(daily, monthly, yearly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_retweet_and_time_df(period='daily',screen_name='Maxis'):\n",
        "    timeline_file1 = 'output/user_timeline_{}.json'.format(screen_name)\n",
        "    dic = {\"date\":[],\"retweet_count\":[]}\n",
        "    with open(timeline_file1) as f1:\n",
        "        for line in f1:\n",
        "            tweet = json.loads(line)\n",
        "            count = tweet['retweet_count']\n",
        "            d = tweet['created_at'].split(' ')\n",
        "            if period=='daily':\n",
        "                d = d[2]+d[1]+d[5]\n",
        "            elif period=='monthly':\n",
        "                d = d[1]+d[5]\n",
        "            elif period=='yearly':\n",
        "                d = d[5]\n",
        "            if d not in dic['date']:\n",
        "                dic['date'].append(d)\n",
        "                dic['retweet_count'].append(count)\n",
        "            else:\n",
        "                idx = dic['date'].index(d)\n",
        "                dic['retweet_count'][idx] += count\n",
        "\n",
        "    return pd.DataFrame(dic)\n",
        "\n",
        "\n",
        "get_retweet_and_time_df(period='daily')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "get_retweet_and_time_df(period='monthly')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "get_retweet_and_time_df(period='yearly')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mOK_L7HTuFLE"
      },
      "source": [
        "Umobile twitter User Retweet count(daily, monthly, yearly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_retweet_and_time_df(period='daily',screen_name='umobile'):\n",
        "    timeline_file1 = 'output/user_timeline_{}.json'.format(screen_name)\n",
        "    dic = {\"date\":[],\"retweet_count\":[]}\n",
        "    with open(timeline_file1) as f1:\n",
        "        for line in f1:\n",
        "            tweet = json.loads(line)\n",
        "            count = tweet['retweet_count']\n",
        "            d = tweet['created_at'].split(' ')\n",
        "            if period=='daily':\n",
        "                d = d[2]+d[1]+d[5]\n",
        "            elif period=='monthly':\n",
        "                d = d[1]+d[5]\n",
        "            elif period=='yearly':\n",
        "                d = d[5]\n",
        "            if d not in dic['date']:\n",
        "                dic['date'].append(d)\n",
        "                dic['retweet_count'].append(count)\n",
        "            else:\n",
        "                idx = dic['date'].index(d)\n",
        "                dic['retweet_count'][idx] += count\n",
        "\n",
        "    return pd.DataFrame(dic)\n",
        "\n",
        "\n",
        "get_retweet_and_time_df(period='daily')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "get_retweet_and_time_df(period='monthly')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "get_retweet_and_time_df(period='yearly')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VOraXAS4uqib"
      },
      "source": [
        "Average and Ratio numbers of Maxis Tweets "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import json\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    screen_name1 = \"Maxis\"\n",
        "    profile_file1 = 'output/users/{}/user_profile.json'.format(screen_name1)\n",
        "    with open(profile_file1) as f1:\n",
        "        profile1 = json.load(f1)\n",
        "        num_tweets = profile1['statuses_count'] \n",
        "\n",
        "    timeline_file1 = 'output/user_timeline_{}.json'.format(screen_name1)\n",
        "\n",
        "    with open(timeline_file1) as f1:\n",
        "        tweet,favorite_count1, retweet_count1 = [], [], []\n",
        "        f_count = 0\n",
        "        r_count = 0\n",
        "        for line in f1:\n",
        "            tweet = json.loads(line)\n",
        "            #print(tweet['text'], \"\\t\", \"Favourite Count: \", tweet['favorite_count'], \"\\t\", \"Retweet_Count: \", tweet['retweet_count'])\n",
        "            f_count += tweet['favorite_count']\n",
        "            r_count += tweet['retweet_count']\n",
        "    print('number of tweets:',num_tweets)\n",
        "    print('average favorite count:',round(f_count/num_tweets,3))\n",
        "    print('average retweet count:',round(r_count/num_tweets,3))\n",
        "    print('ratio (retweet:favorite) = {}:{}'.format(1,round((f_count/num_tweets)/(r_count/num_tweets),5)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fJcANRw3utPb"
      },
      "source": [
        "Average and Ratio numbers of Umobile Tweets "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import json\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    screen_name1 = \"umobile\"\n",
        "    profile_file1 = 'output/users/{}/user_profile.json'.format(screen_name1)\n",
        "    with open(profile_file1) as f1:\n",
        "        profile1 = json.load(f1)\n",
        "        num_tweets = profile1['statuses_count'] \n",
        "\n",
        "    timeline_file1 = 'output/user_timeline_{}.json'.format(screen_name1)\n",
        "\n",
        "    with open(timeline_file1) as f1:\n",
        "        tweet,favorite_count1, retweet_count1 = [], [], []\n",
        "        f_count = 0\n",
        "        r_count = 0\n",
        "        for line in f1:\n",
        "            tweet = json.loads(line)\n",
        "            #print(tweet['text'], \"\\t\", \"Favourite Count: \", tweet['favorite_count'], \"\\t\", \"Retweet_Count: \", tweet['retweet_count'])\n",
        "            f_count += tweet['favorite_count']\n",
        "            r_count += tweet['retweet_count']\n",
        "    print('number of tweets:',num_tweets)\n",
        "    print('average favorite count:',round(f_count/num_tweets,3))\n",
        "    print('average retweet count:',round(r_count/num_tweets,3))\n",
        "    print('ratio (retweet:favorite) = {}:{}'.format(1,round((f_count/num_tweets)/(r_count/num_tweets),5)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yNGIWWmAvEbv"
      },
      "source": [
        "Maxis twitter term frequencies and word cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import sys\n",
        "import string\n",
        "import json\n",
        "from collections import Counter\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def process(text, tokenizer=TweetTokenizer(), stopwords=[]):   \n",
        "    text = text.lower()\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    return [tok for tok in tokens if tok not in stopwords and not tok.isdigit()]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    tweet_tokenizer = TweetTokenizer()\n",
        "    punct = list(string.punctuation)\n",
        "    stopword_list = stopwords.words('english') + punct + ['rt', 'via', '..', '...'] \n",
        "\n",
        "    screen_name1 = \"Maxis\"\n",
        "    fname = 'output/user_timeline_{}.json'.format(screen_name1)\n",
        "\n",
        "    tf = Counter()\n",
        "    all_texts = \"\"\n",
        "    unwanted = ['https','co','rt','…']\n",
        "\n",
        "    with open(fname, 'r') as f:\n",
        "        for line in f:\n",
        "            tweet = json.loads(line)\n",
        "            tokens = process(text=tweet.get('text', ''),\n",
        "                             tokenizer=tweet_tokenizer,\n",
        "                             stopwords=stopword_list)\n",
        "            tf.update(tokens)\n",
        "            for i in tokens:\n",
        "                found = False\n",
        "                for j in unwanted:\n",
        "                    if j in i:\n",
        "                        found = True\n",
        "                        break\n",
        "                if found == False:\n",
        "                    all_texts = all_texts + i + ' '\n",
        "\n",
        "    y = [count for tag, count in tf.most_common(5)]\n",
        "\n",
        "    terms = [t[0] for t in tf.most_common(5)]\n",
        "   \n",
        "    x = range(1, len(y)+1)\n",
        "    x_label = terms\n",
        "\n",
        "    plt.bar(x, y, align='center')\n",
        "    plt.xticks(x,x_label,rotation ='vertical')\n",
        "\n",
        "    #for tag, count in tf.most_common(50):\n",
        "        #print(\"{}: {}\".format(tag, count))\n",
        "\n",
        "    plt.title(\"Term Frequencies\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.savefig(\"term_distribution.png\")\n",
        "    plt.show()\n",
        "\n",
        "    wordcloud = WordCloud(background_color='white', width=5000, height=5000).generate(all_texts)\n",
        "\n",
        "    plt.imshow(wordcloud)\n",
        "    plt.axis(\"on\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mOssK57kvTcO"
      },
      "source": [
        "Umobile twitter Term Frequencies and Wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import sys\n",
        "import string\n",
        "import json\n",
        "from collections import Counter\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def process(text, tokenizer=TweetTokenizer(), stopwords=[]):   \n",
        "    text = text.lower()\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    return [tok for tok in tokens if tok not in stopwords and not tok.isdigit()]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    tweet_tokenizer = TweetTokenizer()\n",
        "    punct = list(string.punctuation)\n",
        "    stopword_list = stopwords.words('english') + punct + ['rt', 'via', '..', '...'] \n",
        "\n",
        "    screen_name1 = \"umobile\"\n",
        "    fname = 'output/user_timeline_{}.json'.format(screen_name1)\n",
        "\n",
        "    tf = Counter()\n",
        "    all_texts = \"\"\n",
        "    unwanted = ['https','co','rt','…']\n",
        "\n",
        "    with open(fname, 'r') as f:\n",
        "        for line in f:\n",
        "            tweet = json.loads(line)\n",
        "            tokens = process(text=tweet.get('text', ''),\n",
        "                             tokenizer=tweet_tokenizer,\n",
        "                             stopwords=stopword_list)\n",
        "            tf.update(tokens)\n",
        "            for i in tokens:\n",
        "                found = False\n",
        "                for j in unwanted:\n",
        "                    if j in i:\n",
        "                        found = True\n",
        "                        break\n",
        "                if found == False:\n",
        "                    all_texts = all_texts + i + ' '\n",
        "\n",
        "    y = [count for tag, count in tf.most_common(5)]\n",
        "\n",
        "    terms = [t[0] for t in tf.most_common(5)]\n",
        "   \n",
        "    x = range(1, len(y)+1)\n",
        "    x_label = terms\n",
        "\n",
        "    plt.bar(x, y, align='center')\n",
        "    plt.xticks(x,x_label,rotation ='vertical')\n",
        "\n",
        "    #for tag, count in tf.most_common(50):\n",
        "        #print(\"{}: {}\".format(tag, count))\n",
        "\n",
        "    plt.title(\"Term Frequencies\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.savefig(\"term_distribution.png\")\n",
        "    plt.show()\n",
        "\n",
        "    wordcloud = WordCloud(background_color='white', width=5000, height=5000).generate(all_texts)\n",
        "\n",
        "    plt.imshow(wordcloud)\n",
        "    plt.axis(\"on\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}